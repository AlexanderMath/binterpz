{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdbarray logmd orb-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e5b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing before.fasta\n"
     ]
    }
   ],
   "source": [
    "%%writefile before.fasta\n",
    ">A|Protein\n",
    "MASPDWGYDDKNGPEQWSKLYPIANGNNQSPVDIKTSETKHDTSLKPISVSYNPATAKEIINVGHSFHVNFEDNDNRSVLKGGPFSDSYRLFQFHFHWGSTNEHGSEHTVDGVKYSAELHVAHWNSAKYSSLAEAASKADGLAVIGVLMKVGEANPKLQKVLDALQAIKTKGKRAPFTNFDPSTLLPSSLDFWTYPGSLTHPPLYESVTWIICKESISVSSEQLAQFRSLLSNVEGDNAVPMQHNNRPTQPLKGRTVRASF\n",
    ">B|SMILES\n",
    "[Zn]\n",
    ">C|SMILES\n",
    "O=C=O\n",
    ">D|SMILES\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785d9536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting after.fasta\n"
     ]
    }
   ],
   "source": [
    "%%writefile after.fasta\n",
    ">A|Protein\n",
    "MASPDWGYDDKNGPEQWSKLYPIANGNNQSPVDIKTSETKHDTSLKPISVSYNPATAKEIINVGHSFHVNFEDNDNRSVLKGGPFSDSYRLFQFHFHWGSTNEHGSEHTVDGVKYSAELHVAHWNSAKYSSLAEAASKADGLAVIGVLMKVGEANPKLQKVLDALQAIKTKGKRAPFTNFDPSTLLPSSLDFWTYPGSLTHPPLYESVTWIICKESISVSSEQLAQFRSLLSNVEGDNAVPMQHNNRPTQPLKGRTVRASF\n",
    ">B|SMILES\n",
    "[Zn]\n",
    ">C|SMILES\n",
    "OC(=O)O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b938863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 'boltz_results_after/lightning_logs/version_0/hparams.yaml'\n",
      "removed directory 'boltz_results_after/lightning_logs/version_0'\n",
      "removed directory 'boltz_results_after/lightning_logs'\n",
      "removed 'boltz_results_after/processed/constraints/after.npz'\n",
      "removed directory 'boltz_results_after/processed/constraints'\n",
      "removed 'boltz_results_after/processed/records/after.json'\n",
      "removed directory 'boltz_results_after/processed/records'\n",
      "removed 'boltz_results_after/processed/mols/after.pkl'\n",
      "removed directory 'boltz_results_after/processed/mols'\n",
      "removed 'boltz_results_after/processed/msa/after_0.npz'\n",
      "removed directory 'boltz_results_after/processed/msa'\n",
      "removed 'boltz_results_after/processed/structures/after.npz'\n",
      "removed directory 'boltz_results_after/processed/structures'\n",
      "removed 'boltz_results_after/processed/manifest.json'\n",
      "removed directory 'boltz_results_after/processed/templates'\n",
      "removed directory 'boltz_results_after/processed'\n",
      "removed 'boltz_results_after/predictions/after/plddt_after_model_0.npz'\n",
      "removed 'boltz_results_after/predictions/after/after_model_0.pdb'\n",
      "removed 'boltz_results_after/predictions/after/pae_after_model_0.npz'\n",
      "removed 'boltz_results_after/predictions/after/confidence_after_model_0.json'\n",
      "removed 'boltz_results_after/predictions/after/pde_after_model_0.npz'\n",
      "removed directory 'boltz_results_after/predictions/after'\n",
      "removed directory 'boltz_results_after/predictions'\n",
      "removed 'boltz_results_after/msa/after_0.csv'\n",
      "removed 'boltz_results_after/msa/after_unpaired_tmp_env/out.tar.gz'\n",
      "removed 'boltz_results_after/msa/after_unpaired_tmp_env/pdb70.m8'\n",
      "removed 'boltz_results_after/msa/after_unpaired_tmp_env/bfd.mgnify30.metaeuk30.smag30.a3m'\n",
      "removed 'boltz_results_after/msa/after_unpaired_tmp_env/uniref.a3m'\n",
      "removed 'boltz_results_after/msa/after_unpaired_tmp_env/msa.sh'\n",
      "removed directory 'boltz_results_after/msa/after_unpaired_tmp_env'\n",
      "removed directory 'boltz_results_after/msa'\n",
      "removed directory 'boltz_results_after/'\n",
      "removed 'boltz_results_before/lightning_logs/version_0/hparams.yaml'\n",
      "removed directory 'boltz_results_before/lightning_logs/version_0'\n",
      "removed directory 'boltz_results_before/lightning_logs'\n",
      "removed 'boltz_results_before/processed/constraints/before.npz'\n",
      "removed directory 'boltz_results_before/processed/constraints'\n",
      "removed 'boltz_results_before/processed/records/before.json'\n",
      "removed directory 'boltz_results_before/processed/records'\n",
      "removed 'boltz_results_before/processed/mols/before.pkl'\n",
      "removed directory 'boltz_results_before/processed/mols'\n",
      "removed 'boltz_results_before/processed/msa/before_0.npz'\n",
      "removed directory 'boltz_results_before/processed/msa'\n",
      "removed 'boltz_results_before/processed/structures/before.npz'\n",
      "removed directory 'boltz_results_before/processed/structures'\n",
      "removed 'boltz_results_before/processed/manifest.json'\n",
      "removed directory 'boltz_results_before/processed/templates'\n",
      "removed directory 'boltz_results_before/processed'\n",
      "removed 'boltz_results_before/predictions/before/confidence_before_model_0.json'\n",
      "removed 'boltz_results_before/predictions/before/pae_before_model_0.npz'\n",
      "removed 'boltz_results_before/predictions/before/plddt_before_model_0.npz'\n",
      "removed 'boltz_results_before/predictions/before/pde_before_model_0.npz'\n",
      "removed 'boltz_results_before/predictions/before/before_model_0.pdb'\n",
      "removed directory 'boltz_results_before/predictions/before'\n",
      "removed directory 'boltz_results_before/predictions'\n",
      "removed 'boltz_results_before/msa/before_unpaired_tmp_env/out.tar.gz'\n",
      "removed 'boltz_results_before/msa/before_unpaired_tmp_env/pdb70.m8'\n",
      "removed 'boltz_results_before/msa/before_unpaired_tmp_env/bfd.mgnify30.metaeuk30.smag30.a3m'\n",
      "removed 'boltz_results_before/msa/before_unpaired_tmp_env/uniref.a3m'\n",
      "removed 'boltz_results_before/msa/before_unpaired_tmp_env/msa.sh'\n",
      "removed directory 'boltz_results_before/msa/before_unpaired_tmp_env'\n",
      "removed 'boltz_results_before/msa/before_0.csv'\n",
      "removed directory 'boltz_results_before/msa'\n",
      "removed directory 'boltz_results_before/'\n",
      "MSA server enabled: https://api.colabfold.com\n",
      "MSA server authentication: no credentials provided\n",
      "Checking input data.\n",
      "Processing 1 inputs with 1 threads.\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]Generating MSA for before.fasta with 1 protein entities.\n",
      "Calling MSA server for target before with 1 sequences\n",
      "MSA server URL: https://api.colabfold.com\n",
      "MSA pairing strategy: greedy\n",
      "No authentication provided for MSA server\n",
      "\n",
      "  0%|                                      | 0/150 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "SUBMIT:   0%|                              | 0/150 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "COMPLETE:   0%|                            | 0/150 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "COMPLETE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [elapsed: 00:03 remaining: 00:00]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.56s/it]\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running structure prediction for 1 input.\n",
      "/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/site-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.5.0.post0, which is newer than your current Lightning version: v2.5.0\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00,  0.07it/s]Number of failed examples: 0ng.pkl`... \n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00,  0.07it/s]\n",
      "MSA server enabled: https://api.colabfold.com\n",
      "MSA server authentication: no credentials provided\n",
      "Checking input data.\n",
      "Processing 1 inputs with 1 threads.\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]Generating MSA for after.fasta with 1 protein entities.\n",
      "Calling MSA server for target after with 1 sequences\n",
      "MSA server URL: https://api.colabfold.com\n",
      "MSA pairing strategy: greedy\n",
      "No authentication provided for MSA server\n",
      "\n",
      "  0%|                                      | 0/150 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "SUBMIT:   0%|                              | 0/150 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "COMPLETE:   0%|                            | 0/150 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "COMPLETE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [elapsed: 00:03 remaining: 00:00]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.58s/it]\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running structure prediction for 1 input.\n",
      "/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/site-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.5.0.post0, which is newer than your current Lightning version: v2.5.0\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00,  0.07it/s]Number of failed examples: 0ng.pkl`... \n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00,  0.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# predict both before/after reaction\n",
    "# the boltz code is modified to store the diffusion module conditioning\n",
    "!rm -rfv boltz_results_after/ boltz_results_before/ \n",
    "!boltz predict before.fasta --no_kernels --output_format pdb --use_msa_server --override #--msa_server_url http://0.0.0.0:8000\n",
    "!boltz predict after.fasta --no_kernels --output_format pdb --use_msa_server --override #--msa_server_url http://0.0.0.0:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb797ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2046, 3) (2046, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">[</span><span style=\"color: #7feb7f; text-decoration-color: #7feb7f\">logmd</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">]</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Load_time</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">=</span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf; font-weight: bold\">0.</span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf\">55s</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> ðŸš€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;2m[\u001b[0m\u001b[2;38;5;40mlogmd\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mLoad_time\u001b[0m\u001b[2m=\u001b[0m\u001b[1;2;34m0\u001b[0m\u001b[1;2;34m.\u001b[0m\u001b[2;34m55s\u001b[0m\u001b[2m ðŸš€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">[</span><span style=\"color: #7feb7f; text-decoration-color: #7feb7f\">logmd</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">]</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Url</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">=</span><a href=\"https://rcsb.ai/c10b9272c3\" target=\"_blank\"><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf; text-decoration: underline\">https://rcsb.ai/c10b9272c3</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> ðŸš€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;2m[\u001b[0m\u001b[2;38;5;40mlogmd\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mUrl\u001b[0m\u001b[2m=\u001b[0m\u001b]8;id=334456;https://rcsb.ai/c10b9272c3\u001b\\\u001b[2;4;34mhttps\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=963956;https://rcsb.ai/c10b9272c3\u001b\\\u001b[2;4;34m://rcsb.ai/c10b9272c3\u001b[0m\u001b]8;;\u001b\\\u001b[2m ðŸš€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' \\nlig_atoms = [int(line[7:11]) for line in str(b).split(\\'\\n\\') if \"LIG\" in line and len(line)>20]\\nnum_lig_atoms = len(lig_atoms)\\nprint(rmsd)\\nA, B = th.tensor(a[-num_lig_atoms:]), th.tensor(b[-num_lig_atoms:])\\nD = th.cdist(A, B)\\n\\nfrom scipy.optimize import linear_sum_assignment; \\nr,c = linear_sum_assignment(D.cpu().numpy()); \\np = th.tensor(c[th.from_numpy(r).argsort().numpy()], device=B.device); \\nprint(p)\\nprint(len(p))\\n\\nb[-num_lig_atoms:] = b.copy()[-num_lig_atoms:][p]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3:\n",
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/alexmath/Desktop/logmd/logmd/logmd.py\", line 175, in upload_worker_process\n",
      "    item = upload_queue.get()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexmath/Desktop/logmd/logmd/logmd.py\", line 175, in upload_worker_process\n",
      "    item = upload_queue.get()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexmath/Desktop/logmd/logmd/logmd.py\", line 175, in upload_worker_process\n",
      "    item = upload_queue.get()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/home/alexmath/miniconda3/envs/binterpz/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# the two boltz predictions will permute/assign different numbering to the ligand atoms. \n",
    "# here we fix this (and inspect manually with logmd) \n",
    "import torch as th \n",
    "import pdbarray as pa \n",
    "\n",
    "b = pa.array('boltz_results_before/predictions/before/before_model_0.pdb')\n",
    "a = pa.array('boltz_results_after/predictions/after/after_model_0.pdb')\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "a, b, rmsd = a.align(b)\n",
    "\n",
    "from logmd import LogMD\n",
    "l = LogMD()\n",
    "l(str(b), input_format='pdb', save_format='bcif')#, data_dict={'orbv3_energy': f\"{E:.2f} [eV]\"})\n",
    "l(str(a), input_format='pdb', save_format='bcif')#, data_dict={'orbv3_energy': f\"{E:.2f} [eV]\"})\n",
    "\n",
    "''' \n",
    "lig_atoms = [int(line[7:11]) for line in str(b).split('\\n') if \"LIG\" in line and len(line)>20]\n",
    "num_lig_atoms = len(lig_atoms)\n",
    "print(rmsd)\n",
    "A, B = th.tensor(a[-num_lig_atoms:]), th.tensor(b[-num_lig_atoms:])\n",
    "D = th.cdist(A, B)\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment; \n",
    "r,c = linear_sum_assignment(D.cpu().numpy()); \n",
    "p = th.tensor(c[th.from_numpy(r).argsort().numpy()], device=B.device); \n",
    "print(p)\n",
    "print(len(p))\n",
    "\n",
    "b[-num_lig_atoms:] = b.copy()[-num_lig_atoms:][p]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce573e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the diffusion module conditioning for `after reaction` \n",
    "# which we apply to `xyz before reaction`\n",
    "import dill \n",
    "self, atom_mask, num_sampling_steps, multiplicity, max_parallel_samples, \\\n",
    "    steering_args, network_condition_kwargs = dill.load(open('diffusion_conditioning_after.fasta.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e566ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜tmpâ€™: File exists\n",
      "mkdir: cannot create directory â€˜tmp/frames/â€™: File exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">[</span><span style=\"color: #7feb7f; text-decoration-color: #7feb7f\">logmd</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">]</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Load_time</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">=</span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf; font-weight: bold\">0.</span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf\">28s</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> ðŸš€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;2m[\u001b[0m\u001b[2;38;5;40mlogmd\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mLoad_time\u001b[0m\u001b[2m=\u001b[0m\u001b[1;2;34m0\u001b[0m\u001b[1;2;34m.\u001b[0m\u001b[2;34m28s\u001b[0m\u001b[2m ðŸš€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">[</span><span style=\"color: #7feb7f; text-decoration-color: #7feb7f\">logmd</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">]</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Url</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">=</span><a href=\"https://rcsb.ai/ec73fbc43e\" target=\"_blank\"><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf; text-decoration: underline\">https://rcsb.ai/ec73fbc43e</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> ðŸš€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;2m[\u001b[0m\u001b[2;38;5;40mlogmd\u001b[0m\u001b[1;2m]\u001b[0m\u001b[2m \u001b[0m\u001b[2;33mUrl\u001b[0m\u001b[2m=\u001b[0m\u001b]8;id=822002;https://rcsb.ai/ec73fbc43e\u001b\\\u001b[2;4;34mhttps\u001b[0m\u001b]8;;\u001b\\\u001b]8;id=29266;https://rcsb.ai/ec73fbc43e\u001b\\\u001b[2;4;34m://rcsb.ai/ec73fbc43e\u001b[0m\u001b]8;;\u001b\\\u001b[2m ðŸš€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de47462330184518988c41c5487a7732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c8cda88a9f49259c5ae6e54fa9bf52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!mkdir tmp \n",
    "!mkdir tmp/frames/\n",
    "from logmd import LogMD\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "import torch \n",
    "import numpy as np \n",
    "import pdbarray as pa \n",
    "import time \n",
    "\n",
    "l = LogMD()\n",
    "energies=  []\n",
    "\n",
    "before = b \n",
    "before = torch.tensor(np.array(before)).cuda()\n",
    "\n",
    "use_nnp = False\n",
    "if use_nnp: \n",
    "    import ase\n",
    "    from ase.build import bulk\n",
    "    from ase.io import read \n",
    "\n",
    "    from orb_models.forcefield import pretrained\n",
    "    from orb_models.forcefield.calculator import ORBCalculator\n",
    "\n",
    "    device=\"cuda\" \n",
    "    orbff = pretrained.orb_v3_direct_20_omat( \n",
    "    device=device,\n",
    "    precision=\"float32-high\",  \n",
    "    )\n",
    "    calc = ORBCalculator(orbff, device=device)\n",
    "    import torch\n",
    "    from ase import Atoms\n",
    "\n",
    "\n",
    "\n",
    "# started from code from https://github.com/lucidrains/alphafold3-pytorch, MIT License, Copyright (c) 2024 Phil Wang\n",
    "from __future__ import annotations\n",
    "import json \n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "import boltz.model.layers.initialize as init\n",
    "from boltz.data import const\n",
    "from boltz.model.loss.diffusionv2 import (\n",
    "    smooth_lddt_loss,\n",
    "    weighted_rigid_align,\n",
    ")\n",
    "from boltz.model.modules.encodersv2 import (\n",
    "    AtomAttentionDecoder,\n",
    "    AtomAttentionEncoder,\n",
    "    SingleConditioning,\n",
    ")\n",
    "from boltz.model.modules.transformersv2 import (\n",
    "    DiffusionTransformer,\n",
    ")\n",
    "from boltz.model.modules.utils import (\n",
    "    LinearNoBias,\n",
    "    center_random_augmentation,\n",
    "    compute_random_augmentation,\n",
    "    default,\n",
    "    log,\n",
    ")\n",
    "\n",
    "if max_parallel_samples is None:\n",
    "    max_parallel_samples = multiplicity\n",
    "\n",
    "num_sampling_steps = default(num_sampling_steps, self.num_sampling_steps)\n",
    "atom_mask = atom_mask.repeat_interleave(multiplicity, 0)\n",
    "\n",
    "shape = (*atom_mask.shape, 3)\n",
    "\n",
    "# get the schedule, which is returned as (sigma, gamma) tuple, and pair up with the next sigma and gamma\n",
    "sigmas = self.sample_schedule(num_sampling_steps)\n",
    "gammas = torch.where(sigmas > self.gamma_min, self.gamma_0, 0.0)\n",
    "sigmas_and_gammas = list(zip(sigmas[:-1], sigmas[1:], gammas[1:]))\n",
    "if self.training and self.step_scale_random is not None:\n",
    "    step_scale = np.random.choice(self.step_scale_random)\n",
    "else:\n",
    "    step_scale = self.step_scale\n",
    "\n",
    "# atom position is noise at the beginning\n",
    "init_sigma = sigmas[0]\n",
    "\n",
    "atom_coords = init_sigma * torch.randn(shape, device=self.device)\n",
    "atom_coords[atom_mask.bool()] = before # \n",
    "#atom_coords = before#.reshape(shape)\n",
    "\n",
    "token_repr = None\n",
    "atom_coords_denoised = None\n",
    "\n",
    "t0 = time.time()\n",
    "Ess = []\n",
    "previous_atom_coords = None\n",
    "\n",
    "for step_idx, (sigma_tm, sigma_t, gamma) in enumerate(tqdm(sigmas_and_gammas)):\n",
    "    #if step_idx < 162: continue  # too little \n",
    "    #if step_idx != 170: continue  # too much \n",
    "    #if step_idx != 162: continue  # too much \n",
    "    #if step_idx != 165: continue  # too much \n",
    "    if step_idx != 168: continue  # too much \n",
    "    sigma_tm, sigma_t, gamma = sigma_tm.item(), sigma_t.item(), gamma.item()\n",
    "    for _ in tqdm(range(50)):\n",
    "\n",
    "        #random_R, random_tr = compute_random_augmentation( multiplicity, device=atom_coords.device, dtype=atom_coords.dtype)\n",
    "        atom_coords = atom_coords - atom_coords.mean(dim=-2, keepdims=True)\n",
    "        #atom_coords = ( torch.einsum(\"bmd,bds->bms\", atom_coords, random_R) + random_tr)\n",
    "\n",
    "        if atom_coords_denoised is not None:\n",
    "            atom_coords_denoised -= atom_coords_denoised.mean(dim=-2, keepdims=True)\n",
    "\n",
    "\n",
    "        t_hat = sigma_tm * (1 + gamma)\n",
    "        steering_t = 1.0 - (step_idx / num_sampling_steps)\n",
    "        noise_var = self.noise_scale**2 * (t_hat**2 - sigma_tm**2)\n",
    "        eps = sqrt(noise_var) * torch.randn(shape, device=self.device)\n",
    "        atom_coords_noisy = atom_coords + eps\n",
    "\n",
    "        with torch.no_grad():\n",
    "            atom_coords_denoised = torch.zeros_like(atom_coords_noisy)\n",
    "            sample_ids = torch.arange(multiplicity).to(atom_coords_noisy.device)\n",
    "            sample_ids_chunks = sample_ids.chunk(\n",
    "                multiplicity % max_parallel_samples + 1\n",
    "            )\n",
    "\n",
    "            for sample_ids_chunk in sample_ids_chunks:\n",
    "                atom_coords_denoised_chunk = self.preconditioned_network_forward(\n",
    "                    atom_coords_noisy[sample_ids_chunk],\n",
    "                    t_hat,\n",
    "                    network_condition_kwargs=dict(\n",
    "                        multiplicity=sample_ids_chunk.numel(),\n",
    "                        **network_condition_kwargs,\n",
    "                    ),\n",
    "                )\n",
    "                atom_coords_denoised[sample_ids_chunk] = atom_coords_denoised_chunk\n",
    "\n",
    "        if self.alignment_reverse_diff and False :\n",
    "            with torch.autocast(\"cuda\", enabled=False):\n",
    "                atom_coords_noisy = weighted_rigid_align(\n",
    "                    atom_coords_noisy.float(),\n",
    "                    atom_coords_denoised.float(),\n",
    "                    atom_mask.float(),\n",
    "                    atom_mask.float(),\n",
    "                )\n",
    "\n",
    "            atom_coords_noisy = atom_coords_noisy.to(atom_coords_denoised)\n",
    "\n",
    "        denoised_over_sigma = (atom_coords_noisy - atom_coords_denoised) / t_hat\n",
    "        atom_coords_next = (\n",
    "            atom_coords_noisy + step_scale * (sigma_t - t_hat) * denoised_over_sigma\n",
    "        )\n",
    "\n",
    "        atom_coords = atom_coords_next\n",
    "\n",
    "\n",
    "        #if False and step_idx>0 and step_idx % 20 == 0 \\\n",
    "        #    and hasattr(self, 'progress') and self.progress is not None and\\\n",
    "        #    hasattr(self, 'writer') and hasattr(self, 'batch') and \\\n",
    "        #    atom_coords.shape[0] == 1: # only show progress when bs=1 for webui\n",
    "        if step_idx > 150: #True: \n",
    "\n",
    "            plot_coords = atom_coords.clone()\n",
    "\n",
    "            out = dict(sample_atom_coords=plot_coords, diff_token_repr=token_repr)\n",
    "            batch = self.batch\n",
    "            pred_dict = {\"exception\": False}\n",
    "\n",
    "            pred_dict[\"masks\"] = batch[\"atom_pad_mask\"]\n",
    "            pred_dict[\"token_masks\"] = batch[\"token_pad_mask\"]\n",
    "\n",
    "            pred_dict[\"coords\"] = out[\"sample_atom_coords\"]\n",
    "            from pathlib import Path\n",
    "            self.writer.output_dir = Path(f\"tmp/\")\n",
    "            self.writer.write_on_batch_end(\n",
    "                None, None, pred_dict, None, self.batch, None, None\n",
    "            )\n",
    "            pdb_str = open(f\"tmp/after/after_model_0.pdb\", 'r').read()\n",
    "            if use_nnp: \n",
    "                from pdbfixer import PDBFixer\n",
    "                from openmm.app import PDBFile\n",
    "\n",
    "                fixer = PDBFixer('tmp/after/after_model_0.pdb')\n",
    "                fixer.addMissingHydrogens(pH=8.4) \n",
    "                PDBFile.writeFile(fixer.topology, fixer.positions, open(f'tmp/with_h.pdb', 'w'))\n",
    "\n",
    "                atoms = read('tmp/with_h.pdb')\n",
    "                H = np.array([atom.index for atom in atoms if atom.symbol == 'H'])\n",
    "                atoms.calc = calc\n",
    "\n",
    "                Es = []\n",
    "                pbar = tqdm(range(10))\n",
    "                for _ in pbar:\n",
    "                    atoms.positions = atoms.positions + 0.01 * atoms.get_forces() \n",
    "                    Es.append(atoms.get_potential_energy())\n",
    "                Ess.append(Es)\n",
    "                atoms.calc = calc\n",
    "                E = atoms.get_potential_energy()\n",
    "            else:\n",
    "                E = 0\n",
    "\n",
    "            energies.append(E)\n",
    "            l(pdb_str, input_format='pdb', save_format='bcif', data_dict={'orbv3_energy': f\"{E:.2f} [eV]\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binterpz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
